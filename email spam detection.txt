import pandas as pd 
import numpy as np
import seaborn as sns 
import matplotlib.pyplot as plt 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, accuracy_score 
from sklearn.metrics import confusion_matrix, accuracy_score 
from sklearn.metrics import classification_report 
df = pd.read_csv('emails.csv')
df


df = pd.read_csv('emails.csv')
df
​
​
Email No.	the	to	ect	and	for	of	a	you	hou	...	connevey	jay	valued	lay	infrastructure	military	allowing	ff	dry	Prediction
0	Email 1	0	0	1	0	0	0	2	0	0	...	0	0	0	0	0	0	0	0	0	0
1	Email 2	8	13	24	6	6	2	102	1	27	...	0	0	0	0	0	0	0	1	0	0
2	Email 3	0	0	1	0	0	0	8	0	0	...	0	0	0	0	0	0	0	0	0	0
3	Email 4	0	5	22	0	5	1	51	2	10	...	0	0	0	0	0	0	0	0	0	0
4	Email 5	7	6	17	1	5	2	57	0	9	...	0	0	0	0	0	0	0	1	0	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
5167	Email 5168	2	2	2	3	0	0	32	0	0	...	0	0	0	0	0	0	0	0	0	0
5168	Email 5169	35	27	11	2	6	5	151	4	3	...	0	0	0	0	0	0	0	1	0	0
5169	Email 5170	0	0	1	1	0	0	11	0	0	...	0	0	0	0	0	0	0	0	0	1
5170	Email 5171	2	7	1	0	2	1	28	2	0	...	0	0	0	0	0	0	0	1	0	1
5171	Email 5172	22	24	5	1	6	5	148	8	2	...	0	0	0	0	0	0	0	0	0	0
5172 rows × 3002 columns

pe
df.shape
(5172, 3002)
df.isnull().any()
Email No.     False
the           False
to            False
ect           False
and           False
              ...  
military      False
allowing      False
ff            False
dry           False
Prediction    False
Length: 3002, dtype: bool
df.drop(columns='Email No.', inplace=True)
df
the	to	ect	and	for	of	a	you	hou	in	...	connevey	jay	valued	lay	infrastructure	military	allowing	ff	dry	Prediction
0	0	0	1	0	0	0	2	0	0	0	...	0	0	0	0	0	0	0	0	0	0
1	8	13	24	6	6	2	102	1	27	18	...	0	0	0	0	0	0	0	1	0	0
2	0	0	1	0	0	0	8	0	0	4	...	0	0	0	0	0	0	0	0	0	0
3	0	5	22	0	5	1	51	2	10	1	...	0	0	0	0	0	0	0	0	0	0
4	7	6	17	1	5	2	57	0	9	3	...	0	0	0	0	0	0	0	1	0	0
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
5167	2	2	2	3	0	0	32	0	0	5	...	0	0	0	0	0	0	0	0	0	0
5168	35	27	11	2	6	5	151	4	3	23	...	0	0	0	0	0	0	0	1	0	0
5169	0	0	1	1	0	0	11	0	0	1	...	0	0	0	0	0	0	0	0	0	1
5170	2	7	1	0	2	1	28	2	0	8	...	0	0	0	0	0	0	0	1	0	1
5171	22	24	5	1	6	5	148	8	2	23	...	0	0	0	0	0	0	0	0	0	0
5172 rows × 3001 columns

df.columns
​
​
Index(['the', 'to', 'ect', 'and', 'for', 'of', 'a', 'you', 'hou', 'in',
       ...
       'connevey', 'jay', 'valued', 'lay', 'infrastructure', 'military',
       'allowing', 'ff', 'dry', 'Prediction'],
      dtype='object', length=3001)
df.Prediction.unique()
​
array([0, 1], dtype=int64)
​
df['Prediction'] = df['Prediction'].replace({0:'Not spam', 1:'Spam'})
df
the	to	ect	and	for	of	a	you	hou	in	...	connevey	jay	valued	lay	infrastructure	military	allowing	ff	dry	Prediction
0	0	0	1	0	0	0	2	0	0	0	...	0	0	0	0	0	0	0	0	0	Not spam
1	8	13	24	6	6	2	102	1	27	18	...	0	0	0	0	0	0	0	1	0	Not spam
2	0	0	1	0	0	0	8	0	0	4	...	0	0	0	0	0	0	0	0	0	Not spam
3	0	5	22	0	5	1	51	2	10	1	...	0	0	0	0	0	0	0	0	0	Not spam
4	7	6	17	1	5	2	57	0	9	3	...	0	0	0	0	0	0	0	1	0	Not spam
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
5167	2	2	2	3	0	0	32	0	0	5	...	0	0	0	0	0	0	0	0	0	Not spam
5168	35	27	11	2	6	5	151	4	3	23	...	0	0	0	0	0	0	0	1	0	Not spam
5169	0	0	1	1	0	0	11	0	0	1	...	0	0	0	0	0	0	0	0	0	Spam
5170	2	7	1	0	2	1	28	2	0	8	...	0	0	0	0	0	0	0	1	0	Spam
5171	22	24	5	1	6	5	148	8	2	23	...	0	0	0	0	0	0	0	0	0	Not spam
5172 rows × 3001 columns

# KNN
X = df.drop(columns='Prediction',axis = 1)
Y = df['Prediction']
X.columns
Index(['the', 'to', 'ect', 'and', 'for', 'of', 'a', 'you', 'hou', 'in',
       ...
       'enhancements', 'connevey', 'jay', 'valued', 'lay', 'infrastructure',
       'military', 'allowing', 'ff', 'dry'],
      dtype='object', length=3000)
Y.head()
0    Not spam
1    Not spam
2    Not spam
3    Not spam
4    Not spam
Name: Prediction, dtype: object
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)
​
from sklearn.metrics import classification_report
​
​
​
KN = KNeighborsClassifier
knn = KN(n_neighbors=7)
knn.fit(x_train, y_train)
y_pred = classifier.predict(X_test)
​
​
print("Prediction: \n")
print(y_pred)
Prediction: 

[1 0 0 ... 1 1 0]
# Accuracy
​
M = metrics.accuracy_score(y_test,y_pred)
print("KNN accuracy: ", M)
KNN accuracy:  0.7990338164251207
C = metrics.confusion_matrix(y_test,y_pred)
print("Confusion matrix: ", C)
Confusion matrix:  [[700  19]
 [189 127]]



# SVM Classifier

model = SVC(C = 1)   # cost C = 1
​
model.fit(x_train, y_train)
​
y_pred = model.predict(x_test)      # predict
kc = metrics.confusion_matrix(y_test, y_pred)
print("SVM accuracy: ", kc)
SVM accuracy:  [[700  19]
 [189 127]]



















Certainly, let's break down the steps for classifying emails into "Normal" (not spam) and "Abnormal" (spam) using binary classification with K-Nearest Neighbors (K-NN) and Support Vector Machine (SVM) methods, and then analyze their performance:

1. **Data Preprocessing**:
   - Start by obtaining and loading the email dataset (emails.csv from Kaggle).
   - Preprocess the data, which includes tasks like removing duplicates, handling missing values, and converting the email content into numerical features. Common techniques like TF-IDF or Bag of Words can be used for this conversion.

2. **Labeling Data**:
   - Assign labels to the emails. Emails marked as "Normal" would be labeled as 0, and those marked as "Abnormal" would be labeled as 1.

3. **Splitting Data**:
   - Split the dataset into a training set and a testing set. This separation allows you to train and evaluate the models separately.

4. **K-Nearest Neighbors (K-NN) Classification**:
   - Train a K-NN model using the training data. K-NN works by looking at the labels of its nearest neighbors in the training data and predicting the label of a new email based on the labels of those neighbors.
   - Choose an appropriate value for 'K,' which represents the number of nearest neighbors to consider.
   - Evaluate the K-NN model's performance using various classification metrics, such as accuracy, precision, recall, and F1-score, on the testing data.

5. **Support Vector Machine (SVM) Classification**:
   - Train an SVM model using the training data. SVM aims to find the best boundary (hyperplane) that separates the "Normal" and "Abnormal" emails with the widest margin.
   - You may need to tune hyperparameters like the type of kernel (e.g., linear or radial basis function) and the regularization parameter.
   - Evaluate the SVM model's performance using the same classification metrics as mentioned for K-NN.

6. **Performance Analysis**:
   - After evaluating both models, compare their performance metrics to determine which one is better at classifying emails as spam or not.
   - Common metrics for binary classification include:
     - **Accuracy:** The proportion of correctly classified emails.
     - **Precision:** The ability of the model to correctly classify "Abnormal" (spam) emails without falsely labeling too many "Normal" emails.
     - **Recall:** The ability of the model to correctly identify most of the "Abnormal" emails without missing many.
     - **F1-score:** A balance between precision and recall, helping you assess a model's overall performance.

7. **Cross-validation (Optional)**:
   - To ensure that the results are robust, you can use techniques like k-fold cross-validation to validate the models on different subsets of the data. This provides a more comprehensive evaluation.

By following these steps, you can classify emails as spam (Abnormal) or not spam (Normal) using K-NN and SVM models and assess their respective performance in doing so. This will help you choose the most suitable method for your email spam detection task.












Sure, let's simplify it:

1. **Data Preparation**: First, get the email data ready. Make sure it's clean and organized. This is like getting your ingredients ready before cooking.

2. **Labeling Emails**: Decide which emails are "normal" (not spam) and which are "abnormal" (spam). It's like putting labels on things to know what they are.

3. **Splitting Data**: Divide your emails into two groups. One group is for teaching the computer (training), and the other is for testing how well it learned.

4. **K-Nearest Neighbors (K-NN)**:
   - Imagine each email is a dot in a big graph. K-NN looks at the closest dots to guess if a new email is normal or spam. You choose how many dots to look at (K).
   - Check if K-NN makes good guesses by using numbers to measure how well it did.

5. **Support Vector Machine (SVM)**:
   - Think of a line that separates normal and spam emails like a fence. SVM finds the best fence that keeps them apart as much as possible.
   - Adjust the fence and see how good it is at separating emails. Use numbers to tell if it's doing a good job.

6. **Performance Check**:
   - After trying both methods, compare them. Look at numbers like accuracy (how many emails were guessed right), precision (how many spam emails were caught without making mistakes), recall (how many spam emails were caught without missing any), and F1-score (an overall measure of how well it's doing).
   - Choose the method that's better at telling normal emails from spam.

7. **Double-Check (Optional)**: To be really sure, you can try the methods multiple times with different sets of emails. It's like tasting your cooking several times to make sure it's always delicious.

This way, you'll figure out which method is better at spotting spam emails, and that's what you'll use.
