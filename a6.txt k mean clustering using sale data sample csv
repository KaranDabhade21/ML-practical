import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns 
import pandas as pd
import seaborn as sns 
from sklearn import preprocessing
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans

from collections import Counter




data = pd.read_csv("C:\\Users\\Admin\\Downloads\\ML Codes\\ML Codes\\sales_data_sample.csv", encoding='Latin-1')
print(data)

print(data.shape) 
 
print(data.isnull().sum() )
 



data.drop(["ORDERNUMBER", "PRICEEACH", "ORDERDATE", "PHONE", "ADDRESSLINE1", "ADDRESSLINE2", "CITY", "STATE", "TERRITORY", "POSTALCODE", "CONTACTLASTNAME", "CONTACTFIRSTNAME"], axis = 1, inplace=True) 
print(data.head() )
print(data.isnull().sum() )
print(data.describe() )



sns.countplot(data = data , x = 'STATUS') 
sns.histplot(x = 'SALES' , hue = 'PRODUCTLINE', data = data,element="poly") 
data['PRODUCTLINE'].unique() 
data.drop_duplicates(inplace=True) 
data.info() 
list_cat = data.select_dtypes(include=['object']).columns.tolist() 
list_cat 
for i in list_cat:  
    sns.countplot(data = data ,x = i)
    plt.xticks(rotation = 90)   
    plt.show() 
 
le = preprocessing.LabelEncoder() 
 
 
for i in list_cat:
    data[i]= le.fit_transform(data[i]) 
 
data.info() 
data['SALES'] = data['SALES'].astype(int) 
data.info() 
data.describe() 
X = data[['SALES','PRODUCTCODE']] 
data.columns 




model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12)).fit(X)
visualizer.show() 
kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(X)
kmeans.labels_  
 
kmeans.inertia_ 
 
kmeans.n_iter_
kmeans.cluster_centers_ 
 

Counter(kmeans.labels_) 
 
sns.scatterplot(data=X, x="SALES", y="PRODUCTCODE", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],marker="X", c="r", s=80, label="centroids") 
plt.legend()
plt.show() 
